{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport argparse\nimport time\nimport cv2\nimport os\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/global-wheat-detection/sample_submission.csv')\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.iloc[0,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load the COCO class labels our YOLO model was trained on\nlabelsPath = '/kaggle/input/global-wheat-detection-yolo-weights/obj.names'\nLABELS = open(labelsPath).read().strip().split(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nCOLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n    dtype=\"uint8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weightsPath = '/kaggle/input/global-wheat-detection-yolo-weights/wheat_detection_tiny_last.weights'\nconfigPath = '/kaggle/input/global-wheat-detection-yolo-weights/wheat_detection_tiny.cfg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[INFO] loading YOLO from disk...\")\nnet = cv2.dnn.readNetFromDarknet(configPath, weightsPath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = '/kaggle/input/global-wheat-detection/test/'\ntest_img_list = os.listdir(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,15))\nfor i,image in enumerate(test_img_list):\n    img = cv2.imread(test + image)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.subplot(2,5,i+1)\n    plt.imshow(img)\n    plt.title(image[:-5])\n    plt.axis('off')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[0,'image_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nfor image_id in range(len(sub)):\n    img = sub.loc[image_id,'image_id']\n    image = cv2.imread(test+img+'.jpg')\n    (H, W) = image.shape[:2]\n\n    thresh = 0.2\n    confi = 0.1\n    pred_str = ''\n    ln = net.getLayerNames()\n    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n    #construct a blob from the input image and then perform a forward\n    #pass of the YOLO object detector, giving us our bounding boxes and\n    #associated probabilities\n    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (512,512),swapRB=True, crop=False)\n    net.setInput(blob)\n    start = time.time()\n    layerOutputs = net.forward(ln)\n    end = time.time()\n\n    #show timing information on YOLO\n    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n\n    #initialize our lists of detected bounding boxes, confidences, and\n    #class IDs, respectively\n    boxes = []\n    confidences = []\n    classIDs = []\n\n    #loop over each of the layer outputs\n    for output in layerOutputs:\n        #loop over each of the detections\n        for detection in output:\n            #extract the class ID and confidence (i.e., probability) of\n            #the current object detection\n            scores = detection[5:]\n            classID = np.argmax(scores)\n            confidence = scores[classID]\n\n            #filter out weak predictions by ensuring the detected\n            #probability is greater than the minimum probability\n            if confidence > confi:\n                #scale the bounding box coordinates back relative to the\n                #size of the image, keeping in mind that YOLO actually\n                #returns the center (x, y)-coordinates of the bounding\n                #box followed by the boxes' width and height\n                box = detection[0:4] * np.array([W, H, W, H])\n                (centerX, centerY, width, height) = box.astype(\"int\")\n\n                #use the center (x, y)-coordinates to derive the top and left corner of the bounding box\n                x = int(centerX - (width / 2))\n                y = int(centerY - (height / 2))\n\n                #update our list of bounding box coordinates, confidences,#and class IDs\n                boxes.append([x, y, int(width), int(height)])\n                confidences.append(float(confidence))\n                classIDs.append(classID)\n\n    #apply non-maxima suppression to suppress weak, overlapping bounding\n    #boxes\n    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confi ,thresh)\n    #ensure at least one detection exists\n    if len(idxs) > 0:\n        #loop over the indexes we are keeping\n        for i in idxs.flatten():\n            #extract the bounding box coordinates\n            (x, y) = (boxes[i][0], boxes[i][1])\n            (w, h) = (boxes[i][2], boxes[i][3])\n            pred = '{} {} {} {} {} '.format(np.round(confidences[i],1),x,y,w,h)\n            pred_str = pred_str + pred\n            \n            #draw a bounding box rectangle and label on the image\n            color = [int(c) for c in COLORS[classIDs[i]]]\n            cv2.rectangle(image, (x, y), (x + w, y + h), (0,0,255), 2)\n            text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 2)\n    \n    \n    sub.loc[image_id,'PredictionString'] = pred_str[:-1]\n    \n\n\n    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    plt.subplot(2,5,image_id+1)\n    plt.imshow(image)\n    plt.title(img)\n    plt.axis('off')\n    \n     \n    \n    \nplt.show()  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}