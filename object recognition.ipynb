{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing necessary libraries\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport time\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Opening file, reading, eliminating whitespaces, and splitting by '\\n', which in turn creates list\nlabels = open('../input/yolo-coco-data/coco.names').read().strip().split('\\n')  # list of names\n\n# # Check point\n# print(labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining paths to the weights and configuration file with model of Neural Network\nweights_path = '../input/yolo-coco-data/yolov3.weights'\nconfiguration_path = '../input/yolo-coco-data/yolov3.cfg'\n\n# Setting minimum probability to eliminate weak predictions\nprobability_minimum = 0.5\n\n# Setting threshold for non maximum suppression\nthreshold = 0.3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"network = cv2.dnn.readNetFromDarknet(configuration_path, weights_path)\n\n# Getting names of all layers\nlayers_names_all = network.getLayerNames()  # list of layers' names\n\n# # Check point\n# print(layers_names_all)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting only output layers' names that we need from YOLO algorithm\nlayers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]  # list of layers' names\n\n# Check point\nprint(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our image initially is in RGB format\n# But now we open it in BGR format as function 'cv2.imread' opens it so\nimage_input = cv2.imread('../input/images-for-testing/cat2.jpg')\n\n# Getting image shape\nimage_input_shape = image_input.shape\n\n# Check point\nprint(image_input_shape)  # tuple of (917, 1222, 3)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Showing RGB image but firstly converting it from BGR format\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob\n# from input image after mean subtraction, normalizing, and RB channels swapping\n# Resulted shape has number of images, number of channels, width and height\n# E.G.: blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n# Link: https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\nblob = cv2.dnn.blobFromImage(image_input, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n# Check point\nprint(image_input.shape)  # (917, 1222, 3)\nprint(blob.shape)  # (1, 3, 416, 416)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check point\n# Slicing blob and transposing to make channels come at the end\nblob_to_show = blob[0, :, :, :].transpose(1, 2, 0)\nprint(blob_to_show.shape)  # (416, 416, 3)\n\n# Showing 'blob_to_show'\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 5.0)\nplt.imshow(blob_to_show)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating at the same time, needed time for forward pass\nnetwork.setInput(blob)  # setting blob as input to the network\nstart = time.time()\noutput_from_network = network.forward(layers_names_output)\nend = time.time()\n\n# Showing spent time for forward pass\nprint('YOLO v3 took {:.5f} seconds'.format(end - start))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check point\nprint(type(output_from_network))  # <class 'list'>\nprint(type(output_from_network[0]))  # <class 'numpy.ndarray'>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed the generator - every time we run the code it will generate by the same rules\n# In this way we can keep specific colour the same for every class\nnp.random.seed(42)\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n\n# Check point\nprint(colours.shape)  # (80, 3)\nprint(colours[0])  # [102 220 225]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing lists for detected bounding boxes, obtained confidences and class's number\nbounding_boxes = []\nconfidences = []\nclass_numbers = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting spacial dimension of input image\nh, w = image_input_shape[:2]  # Slicing from tuple only first two elements\n\n# Check point\nprint(h, w)  # 917 1222","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for result in output_from_network:\n    # Going through all detections from current output layer\n    for detection in result:\n        # Getting class for current object\n        scores = detection[5:]\n        class_current = np.argmax(scores)\n\n        # Getting confidence (probability) for current object\n        confidence_current = scores[class_current]\n\n        # Eliminating weak predictions by minimum probability\n        if confidence_current > probability_minimum:\n            # Scaling bounding box coordinates to the initial image size\n            # YOLO data format keeps center of detected box and its width and height\n            # That is why we can just elementwise multiply them to the width and height of the image\n            box_current = detection[0:4] * np.array([w, h, w, h])\n\n            # From current box with YOLO format getting top left corner coordinates\n            # that are x_min and y_min\n            x_center, y_center, box_width, box_height = box_current.astype('int')\n            x_min = int(x_center - (box_width / 2))\n            y_min = int(y_center - (box_height / 2))\n\n            # Adding results into prepared lists\n            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n            confidences.append(float(confidence_current))\n            class_numbers.append(class_current)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is needed to make sure the data type of the boxes is 'int'\n# and the type of the confidences is 'float'\n# https://github.com/opencv/opencv/issues/12789\nresults = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n# Check point\n# Showing labels of the detected objects\nfor i in range(len(class_numbers)):\n    print(labels[int(class_numbers[i])])\n\n# Saving found labels\nwith open('found_labels.txt', 'w') as f:\n    for i in range(len(class_numbers)):\n        f.write(labels[int(class_numbers[i])])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if there is at least one detected object\nif len(results) > 0:\n    # Going through indexes of results\n    for i in results.flatten():\n        # Getting current bounding box coordinates\n        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n        # Preparing colour for current bounding box\n        colour_box_current = [int(j) for j in colours[class_numbers[i]]]\n\n        # Drawing bounding box on the original image\n        cv2.rectangle(image_input, (x_min, y_min), (x_min + box_width, y_min + box_height),\n                      colour_box_current, 5)\n\n        # Preparing text with label and confidence for current bounding box\n        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])], confidences[i])\n\n        # Putting text with label and confidence on the original image\n        cv2.putText(image_input, text_box_current, (x_min, y_min - 7), cv2.FONT_HERSHEY_SIMPLEX,\n                    1.5, colour_box_current, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}